{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6vMCaTzdu6a"
      },
      "source": [
        "# In this Notebook we will learn how we can use different documents and get them analysed by GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VROza_wGd4if"
      },
      "source": [
        "## First of all we will install the required packages that will help us along the way\n",
        "1. openai: This package will help us to call the chat completion method of openai to generate results using GPT.\n",
        "3. PyMuPDF: This package is used for easy PDF manipulation.\n",
        "4. tiktoken: This package is used to calculate the tokens in a text\n",
        "\n",
        "### üìå Prerequisites  \n",
        "\n",
        "Please download and review the following documents before proceeding:  \n",
        "\n",
        "1. **AWS1.pdf**  \n",
        "   [Download Link](https://drive.google.com/file/d/1XSe2pSsGN1ssAbif92rvb80AnHb_Ni0F/view?usp=sharing)  \n",
        "\n",
        "2. **PROFRAC HOLDINGS, LLC Credit Agreement.pdf**  \n",
        "   [Download Link](https://drive.google.com/file/d/1UyOxeaEQsK5TFxXHI63PshoKmj0yjTmW/view?usp=sharing)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cgny4E8V8NGU"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "!pip install PyMuPDF==1.24.2 PyMuPDFb==1.24.1 tqdm tiktoken\n",
        "! pip install openai==1.55.3 httpx==0.27.2 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pKRpv0MSNnln"
      },
      "outputs": [],
      "source": [
        "! pip install openai httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20WwGnF0kLPB"
      },
      "source": [
        "# Make Sure to Place Your OPEN API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H6Z8_eO38qlS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=\"<insert key>\")\n",
        "\n",
        "\n",
        "token_encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErbsjVzCgLcG"
      },
      "source": [
        "Here we have created a function that is called for generating response from OpenAI\n",
        "\n",
        "This loads the keys from the environment and uses it to call the `openai.chat.completions.create` method\n",
        "\n",
        "This method takes the system message and the user message as input for generating response.\n",
        "\n",
        "### The temperature parameter defines the randomness of the output,\n",
        "\n",
        "Higher the temperature the more creative the LLM will become with its answers, thus higher temperatures are used for poem generation, jokes etc.\n",
        "Lower temperature gives deterministic results and return the most probable next token.\n",
        "\n",
        "### Top P\n",
        "\n",
        "A sampling technique with temperature, called nucleus sampling, where you can control how deterministic the model is. If you are looking for exact and factual answers keep this low. If you are looking for more diverse responses, increase to a higher value. If you use Top P it means that only the tokens comprising the top_p probability mass are considered for responses, so a low top_p value selects the most confident responses. This means that a high top_p value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXJBE_y19pCq"
      },
      "outputs": [],
      "source": [
        "def CallOpenAI(user,system):\n",
        "  response = client.chat.completions.create(\n",
        "              model= \"gpt-3.5-turbo\", # model = \"deployment_name\".\n",
        "              temperature= 0,\n",
        "              top_p= 0,\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": system},\n",
        "                  {\"role\": \"user\", \"content\": user}\n",
        "              ]\n",
        "          )\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YvMRDm-Fx5"
      },
      "source": [
        "## Lets take a contract and try to analyse it without much instruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-JwKDwhgpvm"
      },
      "source": [
        "First we load the PDF and extract the texts from it and generate the token count of the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI9uVaob_7Ie"
      },
      "outputs": [],
      "source": [
        "def extract_text(pdf_path):\n",
        "  pdf = fitz.open(pdf_path)\n",
        "  text = ''\n",
        "\n",
        "  for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "  num_tokens = len(token_encoding.encode(text))\n",
        "  print(\"Number of tokens in the entire Document: \", num_tokens)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtScsyS8gz0r"
      },
      "source": [
        "Out here we can see the token count of the document is 11590 which is well withing the 16000 context limit of the GPT-3.5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj_16PHCKS1R"
      },
      "source": [
        "**‚ö†Ô∏è Note:** **In the cell below, you need to upload a file named `AWS1.pdf`.**  \n",
        "**You can download the file from the link below.**\n",
        "[üì• Download AWS1.pdf](https://drive.google.com/file/d/1XSe2pSsGN1ssAbif92rvb80AnHb_Ni0F/view?usp=sharing)/Lab-2.1(Generating-Response-without-RAG)/AWS1.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aTg1NEj_H83Q"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-vel8SEHS_C",
        "outputId": "7e4963d5-b97d-4ff3-9541-df903ba42127"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens in the entire Document:  11590\n"
          ]
        }
      ],
      "source": [
        "short_document = extract_text(\"/content/AWS1.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtnxFpZyg9gr"
      },
      "source": [
        "## We concatenate the text from the PDF and the question that the user wants to ask to the GPT about the PDF and form a prompt that we will use to generate the response using `openai.chat.completion.create` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJlC7zoDHmpV"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the governing courts for Amazon Web Services South Africa ProprietaryLimited\"\n",
        "\n",
        "full_prompt_SD = \"<Context>\"+short_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "097gI1cFKWHu"
      },
      "outputs": [],
      "source": [
        "response = CallOpenAI(full_prompt_SD,\"You are a Professional lawyer who can analyse documents thorougly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT-ynR-nhSdx"
      },
      "source": [
        "### We can see that the GPT was able to generate the answer by refering to the prompt and give the correct result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZjJRGvhK_Ra",
        "outputId": "67c99ac6-122d-4ccb-bb0d-f71ac05d73d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The governing courts for Amazon Web Services South Africa Proprietary Limited are the South Gauteng High Court in Johannesburg, South Africa. This information is specified in the document under the \"AWS Contracting Party\" section for South Africa. The document outlines the laws and courts applicable to each AWS Contracting Party based on the Account Country associated with the AWS account.\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_rAndtLKzTq"
      },
      "source": [
        "## Now lets load up a document that has more than 16000 tokens, which is the limit of GPT-3.5-Turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0gbRdAnK9F3"
      },
      "source": [
        "**‚ö†Ô∏è Note:** **In the cell below, you need to upload a file named `PROFRAC HOLDINGS, LLC credit agreement.pdf`.**  \n",
        "**You can download the file from the link below.**\n",
        "[üì• Download PROFRAC HOLDINGS, LLC credit agreement.pdf ](https://drive.google.com/file/d/1UyOxeaEQsK5TFxXHI63PshoKmj0yjTmW/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DwqltHhiK_gT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "04e1c5a4-5bc8-442f-8120-e87ecbe75b3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-55be8c39-ce4a-4356-99f0-9809194c00cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-55be8c39-ce4a-4356-99f0-9809194c00cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PROFRAC HOLDINGS, LLC credit agreement.pdf to PROFRAC HOLDINGS, LLC credit agreement.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja6FHWxeAiKo",
        "outputId": "1a56ff0b-12da-4164-db96-3310de8c027e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the entire Document:  163227\n"
          ]
        }
      ],
      "source": [
        "long_document = extract_text(\"/content/PROFRAC HOLDINGS, LLC credit agreement.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dfpmAVqmKCXM"
      },
      "outputs": [],
      "source": [
        "Question = \"What is the Acknowledgement Regarding Any Supported QFCs?\"\n",
        "\n",
        "full_prompt_LD = \"<Context>\"+long_document+\"</Context>\" +\"\\n\\n\" +\"<Question>\"+Question+\"</Question>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve7bc33tLygQ"
      },
      "source": [
        "## Here what you see is, when the message length exceeded the limit of GPT, it throws an error.\n",
        "### This problem will be fixed in the next lab where you see how Retrieval Augmented Generation(RAG) will fix this problem and enable us to analyse documents of any length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBWHfMohKxKw",
        "outputId": "9828df05-2de1-415d-ef13-fd224c38b74a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 163274 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response = CallOpenAI(full_prompt_LD,\"You are a Professional lawyer who can analyse documents thorougly\")\n",
        "except Exception as e:\n",
        "  print(str(e))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}